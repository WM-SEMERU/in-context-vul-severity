{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch as t\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity(40)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program variables\n",
    "max_iterations = 10\n",
    "conversation_history = list()\n",
    "model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "filename = f\"{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if t.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir =\"../datax/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67de916b6b954000a2b0f07029050971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, cache_dir=cache_dir, device_map=device, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, cache_dir=cache_dir, padding_side=\"left\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32016, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (28): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (29): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (30): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "      (31): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_template= \"The following snippet is a exploitable code with a score of {}  {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_template = \"What is the score for the following snippet?  {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = '''The following snippet is a exploitable code with a score of 4.5\n",
    "\tdev = usb_get_intfdata(interface);\n",
    "\tif (!dev) {\n",
    "\t\tretval = -ENODEV;\n",
    "\t\tgoto exit;\n",
    "\t}\n",
    "\n",
    "\t/* increment our usage count for the device */\n",
    "\tkref_get(&dev->kref);\n",
    "\n",
    "\t/* save our object in the file's private structure */\n",
    "\tmutex_lock(&dev->io_mutex);\n",
    "\tfile->private_data = dev;\n",
    "\tmutex_unlock(&dev->io_mutex);'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = '''The following snippet has a score of 7.5: int i;\n",
    "char inLine[64];\n",
    "cin >> inLine;\n",
    "i = atoi (inLine);\n",
    "sleep(i);'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = '''What is the score for the following snippet? \n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "\trc = SQLConnect(Example.ConHandle, argv[0], SQL_NTS,\n",
    "\t(SQLCHAR *) \"\", SQL_NTS, (SQLCHAR *) \"\", SQL_NTS);\n",
    "} '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MESSAGES_LENGTH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(\"../datax/big-vul/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Access Gained</th>\n",
       "      <th>Attack Origin</th>\n",
       "      <th>Authentication Required</th>\n",
       "      <th>Availability</th>\n",
       "      <th>CVE ID</th>\n",
       "      <th>CVE Page</th>\n",
       "      <th>CWE ID</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Confidentiality</th>\n",
       "      <th>...</th>\n",
       "      <th>parentID</th>\n",
       "      <th>patch</th>\n",
       "      <th>project</th>\n",
       "      <th>project_after</th>\n",
       "      <th>project_before</th>\n",
       "      <th>target</th>\n",
       "      <th>vul_func_with_fix</th>\n",
       "      <th>processed_func</th>\n",
       "      <th>flaw_line</th>\n",
       "      <th>flaw_line_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVE-2014-3508</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2014-3508/</td>\n",
       "      <td>CWE-200</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>17160033765480453be0a41335fa6b833691c049</td>\n",
       "      <td>@@ -471,11 +471,12 @@ int OBJ_obj2txt(char *bu...</td>\n",
       "      <td>openssl</td>\n",
       "      <td>https://git.openssl.org/gitweb/?p=openssl.git;...</td>\n",
       "      <td>https://git.openssl.org/gitweb/?p=openssl.git;...</td>\n",
       "      <td>0</td>\n",
       "      <td>const char *OBJ_nid2sn(int n)\\n\\t{\\n\\tADDED_OB...</td>\n",
       "      <td>const char *OBJ_nid2sn(int n)\\n\\t{\\n\\tADDED_OB...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local</td>\n",
       "      <td>Not required</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVE-2011-4080</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2011-4080/</td>\n",
       "      <td>CWE-264</td>\n",
       "      <td>High</td>\n",
       "      <td>Complete</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -170,6 +170,11 @@ static int proc_taint(str...</td>\n",
       "      <td>linux</td>\n",
       "      <td>bfdc0b497faa82a0ba2f9dddcf109231dd519fcc</td>\n",
       "      <td>cb16e95fa2996743a6e80a665ed2ed0590bd38cf</td>\n",
       "      <td>0</td>\n",
       "      <td>void register_sysctl_root(struct ctl_table_roo...</td>\n",
       "      <td>void register_sysctl_root(struct ctl_table_roo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Partial</td>\n",
       "      <td>CVE-2012-2875</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2012-2875/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -4058,11 +4058,6 @@ bool WebPage::touchEven...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>d345af9ed62ee5f431be327967f41c3cc3fe936a</td>\n",
       "      <td>e261bb8e47a6a9fdd1d26fd52b1538c5c9bcb122</td>\n",
       "      <td>0</td>\n",
       "      <td>WebPagePrivate::~WebPagePrivate()\\n{\\n    // H...</td>\n",
       "      <td>WebPagePrivate::~WebPagePrivate()\\n{\\n    m_we...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2009-3604</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2009-3604/</td>\n",
       "      <td>CWE-399</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Complete</td>\n",
       "      <td>...</td>\n",
       "      <td>75c3466ba2e4980802e80b939495981240261cd5</td>\n",
       "      <td>@@ -216,6 +216,28 @@ void *gmallocn_checkoverf...</td>\n",
       "      <td>poppler</td>\n",
       "      <td>https://cgit.freedesktop.org/poppler/poppler/t...</td>\n",
       "      <td>https://cgit.freedesktop.org/poppler/poppler/t...</td>\n",
       "      <td>0</td>\n",
       "      <td>char *gstrndup(const char *s, size_t n) {\\n  c...</td>\n",
       "      <td>char *gstrndup(const char *s, size_t n) {\\n  c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVE-2019-15164</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2019-15164/</td>\n",
       "      <td>CWE-918</td>\n",
       "      <td>Low</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -156,6 +156,8 @@ static int rpcapd_recv(SOC...</td>\n",
       "      <td>libpcap</td>\n",
       "      <td>33834cb2a4d035b52aa2a26742f832a112e90a0a</td>\n",
       "      <td>484d60cbf7ca4ec758c3cbb8a82d68b244a78d58</td>\n",
       "      <td>0</td>\n",
       "      <td>daemon_AuthUserPwd(char *username, char *passw...</td>\n",
       "      <td>daemon_AuthUserPwd(char *username, char *passw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index Access Gained Attack Origin Authentication Required Availability  \\\n",
       "0   12473           NaN        Remote            Not required          NaN   \n",
       "1   24444           NaN         Local            Not required          NaN   \n",
       "2  111436           NaN        Remote            Not required      Partial   \n",
       "3    1314           NaN        Remote            Not required     Complete   \n",
       "4   88406           NaN        Remote            Not required          NaN   \n",
       "\n",
       "           CVE ID                                        CVE Page   CWE ID  \\\n",
       "0   CVE-2014-3508   https://www.cvedetails.com/cve/CVE-2014-3508/  CWE-200   \n",
       "1   CVE-2011-4080   https://www.cvedetails.com/cve/CVE-2011-4080/  CWE-264   \n",
       "2   CVE-2012-2875   https://www.cvedetails.com/cve/CVE-2012-2875/      NaN   \n",
       "3   CVE-2009-3604   https://www.cvedetails.com/cve/CVE-2009-3604/  CWE-399   \n",
       "4  CVE-2019-15164  https://www.cvedetails.com/cve/CVE-2019-15164/  CWE-918   \n",
       "\n",
       "  Complexity Confidentiality  ...                                  parentID  \\\n",
       "0     Medium         Partial  ...  17160033765480453be0a41335fa6b833691c049   \n",
       "1       High        Complete  ...                                       NaN   \n",
       "2     Medium         Partial  ...                                       NaN   \n",
       "3     Medium        Complete  ...  75c3466ba2e4980802e80b939495981240261cd5   \n",
       "4        Low             NaN  ...                                       NaN   \n",
       "\n",
       "                                               patch  project  \\\n",
       "0  @@ -471,11 +471,12 @@ int OBJ_obj2txt(char *bu...  openssl   \n",
       "1  @@ -170,6 +170,11 @@ static int proc_taint(str...    linux   \n",
       "2  @@ -4058,11 +4058,6 @@ bool WebPage::touchEven...   Chrome   \n",
       "3  @@ -216,6 +216,28 @@ void *gmallocn_checkoverf...  poppler   \n",
       "4  @@ -156,6 +156,8 @@ static int rpcapd_recv(SOC...  libpcap   \n",
       "\n",
       "                                       project_after  \\\n",
       "0  https://git.openssl.org/gitweb/?p=openssl.git;...   \n",
       "1           bfdc0b497faa82a0ba2f9dddcf109231dd519fcc   \n",
       "2           d345af9ed62ee5f431be327967f41c3cc3fe936a   \n",
       "3  https://cgit.freedesktop.org/poppler/poppler/t...   \n",
       "4           33834cb2a4d035b52aa2a26742f832a112e90a0a   \n",
       "\n",
       "                                      project_before target  \\\n",
       "0  https://git.openssl.org/gitweb/?p=openssl.git;...      0   \n",
       "1           cb16e95fa2996743a6e80a665ed2ed0590bd38cf      0   \n",
       "2           e261bb8e47a6a9fdd1d26fd52b1538c5c9bcb122      0   \n",
       "3  https://cgit.freedesktop.org/poppler/poppler/t...      0   \n",
       "4           484d60cbf7ca4ec758c3cbb8a82d68b244a78d58      0   \n",
       "\n",
       "                                   vul_func_with_fix  \\\n",
       "0  const char *OBJ_nid2sn(int n)\\n\\t{\\n\\tADDED_OB...   \n",
       "1  void register_sysctl_root(struct ctl_table_roo...   \n",
       "2  WebPagePrivate::~WebPagePrivate()\\n{\\n    // H...   \n",
       "3  char *gstrndup(const char *s, size_t n) {\\n  c...   \n",
       "4  daemon_AuthUserPwd(char *username, char *passw...   \n",
       "\n",
       "                                      processed_func flaw_line flaw_line_index  \n",
       "0  const char *OBJ_nid2sn(int n)\\n\\t{\\n\\tADDED_OB...       NaN             NaN  \n",
       "1  void register_sysctl_root(struct ctl_table_roo...       NaN             NaN  \n",
       "2  WebPagePrivate::~WebPagePrivate()\\n{\\n    m_we...       NaN             NaN  \n",
       "3  char *gstrndup(const char *s, size_t n) {\\n  c...       NaN             NaN  \n",
       "4  daemon_AuthUserPwd(char *username, char *passw...       NaN             NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_val = val_data[val_data['func_before'].str.len() < 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../datax/big-vul/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test = test_data[test_data['func_before'].str.len() < 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Access Gained</th>\n",
       "      <th>Attack Origin</th>\n",
       "      <th>Authentication Required</th>\n",
       "      <th>Availability</th>\n",
       "      <th>CVE ID</th>\n",
       "      <th>CVE Page</th>\n",
       "      <th>CWE ID</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Confidentiality</th>\n",
       "      <th>...</th>\n",
       "      <th>parentID</th>\n",
       "      <th>patch</th>\n",
       "      <th>project</th>\n",
       "      <th>project_after</th>\n",
       "      <th>project_before</th>\n",
       "      <th>target</th>\n",
       "      <th>vul_func_with_fix</th>\n",
       "      <th>processed_func</th>\n",
       "      <th>flaw_line</th>\n",
       "      <th>flaw_line_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2016-6561</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2016-6561/</td>\n",
       "      <td>CWE-476</td>\n",
       "      <td>Low</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -21,6 +21,7 @@\\n /*\\n  * Copyright (c) 2007...</td>\n",
       "      <td>illumos-gate</td>\n",
       "      <td>6d1c73b5858fefc6161c7d686345f0dc887ea799</td>\n",
       "      <td>516627f338a630bcf9806a91aa873bbbae9a2fac</td>\n",
       "      <td>0</td>\n",
       "      <td>smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...</td>\n",
       "      <td>smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2016-3138</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2016-3138/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -1179,6 +1179,9 @@ static int acm_probe(str...</td>\n",
       "      <td>linux</td>\n",
       "      <td>8835ba4a39cf53f705417b3b3a94eb067673f2c9</td>\n",
       "      <td>0b818e3956fc1ad976bee791eadcbb3b5fec5bfd</td>\n",
       "      <td>0</td>\n",
       "      <td>static inline int acm_set_control(struct acm *...</td>\n",
       "      <td>static inline int acm_set_control(struct acm *...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVE-2018-6145</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2018-6145/</td>\n",
       "      <td>CWE-79</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -82,13 +82,6 @@ static bool TokenExitsForei...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>133bc5c262b2555af223263452e9875a95db9eb7</td>\n",
       "      <td>1e8327c88920544f1503004b4e32850c935d4efb</td>\n",
       "      <td>0</td>\n",
       "      <td>HTMLTreeBuilderSimulator::State HTMLTreeBuilde...</td>\n",
       "      <td>HTMLTreeBuilderSimulator::State HTMLTreeBuilde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Partial</td>\n",
       "      <td>CVE-2012-5135</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2012-5135/</td>\n",
       "      <td>CWE-399</td>\n",
       "      <td>Low</td>\n",
       "      <td>Partial</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -713,7 +713,8 @@ PrintWebViewHelper::PrintW...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>b755ebba29dd405d6f1e4cf70f5bc81ffd33b0f6</td>\n",
       "      <td>7b688dec9fa8ab42a4933e381ad9aeb63413139b</td>\n",
       "      <td>0</td>\n",
       "      <td>int PrintWebViewHelper::PrintPreviewContext::t...</td>\n",
       "      <td>int PrintWebViewHelper::PrintPreviewContext::t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local</td>\n",
       "      <td>Not required</td>\n",
       "      <td>Complete</td>\n",
       "      <td>CVE-2018-16276</td>\n",
       "      <td>https://www.cvedetails.com/cve/CVE-2018-16276/</td>\n",
       "      <td>CWE-20</td>\n",
       "      <td>Low</td>\n",
       "      <td>Complete</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@@ -396,35 +396,24 @@ static ssize_t yurex_rea...</td>\n",
       "      <td>linux</td>\n",
       "      <td>f1e255d60ae66a9f672ff9a207ee6cd8e33d2679</td>\n",
       "      <td>bba57eddadda936c94b5dccf73787cb9e159d0a5</td>\n",
       "      <td>0</td>\n",
       "      <td>static int yurex_open(struct inode *inode, str...</td>\n",
       "      <td>static int yurex_open(struct inode *inode, str...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index Access Gained Attack Origin Authentication Required Availability  \\\n",
       "0   73752           NaN        Remote            Not required     Complete   \n",
       "1   54196           NaN         Local            Not required     Complete   \n",
       "2  169124           NaN        Remote            Not required          NaN   \n",
       "3  109551           NaN        Remote            Not required      Partial   \n",
       "4   78906           NaN         Local            Not required     Complete   \n",
       "\n",
       "           CVE ID                                        CVE Page   CWE ID  \\\n",
       "0   CVE-2016-6561   https://www.cvedetails.com/cve/CVE-2016-6561/  CWE-476   \n",
       "1   CVE-2016-3138   https://www.cvedetails.com/cve/CVE-2016-3138/      NaN   \n",
       "2   CVE-2018-6145   https://www.cvedetails.com/cve/CVE-2018-6145/   CWE-79   \n",
       "3   CVE-2012-5135   https://www.cvedetails.com/cve/CVE-2012-5135/  CWE-399   \n",
       "4  CVE-2018-16276  https://www.cvedetails.com/cve/CVE-2018-16276/   CWE-20   \n",
       "\n",
       "  Complexity Confidentiality  ... parentID  \\\n",
       "0        Low             NaN  ...      NaN   \n",
       "1        Low             NaN  ...      NaN   \n",
       "2     Medium             NaN  ...      NaN   \n",
       "3        Low         Partial  ...      NaN   \n",
       "4        Low        Complete  ...      NaN   \n",
       "\n",
       "                                               patch       project  \\\n",
       "0  @@ -21,6 +21,7 @@\\n /*\\n  * Copyright (c) 2007...  illumos-gate   \n",
       "1  @@ -1179,6 +1179,9 @@ static int acm_probe(str...         linux   \n",
       "2  @@ -82,13 +82,6 @@ static bool TokenExitsForei...        Chrome   \n",
       "3  @@ -713,7 +713,8 @@ PrintWebViewHelper::PrintW...        Chrome   \n",
       "4  @@ -396,35 +396,24 @@ static ssize_t yurex_rea...         linux   \n",
       "\n",
       "                              project_after  \\\n",
       "0  6d1c73b5858fefc6161c7d686345f0dc887ea799   \n",
       "1  8835ba4a39cf53f705417b3b3a94eb067673f2c9   \n",
       "2  133bc5c262b2555af223263452e9875a95db9eb7   \n",
       "3  b755ebba29dd405d6f1e4cf70f5bc81ffd33b0f6   \n",
       "4  f1e255d60ae66a9f672ff9a207ee6cd8e33d2679   \n",
       "\n",
       "                             project_before target  \\\n",
       "0  516627f338a630bcf9806a91aa873bbbae9a2fac      0   \n",
       "1  0b818e3956fc1ad976bee791eadcbb3b5fec5bfd      0   \n",
       "2  1e8327c88920544f1503004b4e32850c935d4efb      0   \n",
       "3  7b688dec9fa8ab42a4933e381ad9aeb63413139b      0   \n",
       "4  bba57eddadda936c94b5dccf73787cb9e159d0a5      0   \n",
       "\n",
       "                                   vul_func_with_fix  \\\n",
       "0  smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...   \n",
       "1  static inline int acm_set_control(struct acm *...   \n",
       "2  HTMLTreeBuilderSimulator::State HTMLTreeBuilde...   \n",
       "3  int PrintWebViewHelper::PrintPreviewContext::t...   \n",
       "4  static int yurex_open(struct inode *inode, str...   \n",
       "\n",
       "                                      processed_func flaw_line flaw_line_index  \n",
       "0  smb_ofile_delete(void *arg)\\n{\\n\\tsmb_tree_t\\t...       NaN             NaN  \n",
       "1  static inline int acm_set_control(struct acm *...       NaN             NaN  \n",
       "2  HTMLTreeBuilderSimulator::State HTMLTreeBuilde...       NaN             NaN  \n",
       "3  int PrintWebViewHelper::PrintPreviewContext::t...       NaN             NaN  \n",
       "4  static int yurex_open(struct inode *inode, str...       NaN             NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index',\n",
       " 'Access Gained',\n",
       " 'Attack Origin',\n",
       " 'Authentication Required',\n",
       " 'Availability',\n",
       " 'CVE ID',\n",
       " 'CVE Page',\n",
       " 'CWE ID',\n",
       " 'Complexity',\n",
       " 'Confidentiality',\n",
       " 'Integrity',\n",
       " 'Known Exploits',\n",
       " 'Publish Date',\n",
       " 'Score',\n",
       " 'Summary',\n",
       " 'Update Date',\n",
       " 'Vulnerability Classification',\n",
       " 'add_lines',\n",
       " 'codeLink',\n",
       " 'commit_id',\n",
       " 'commit_message',\n",
       " 'del_lines',\n",
       " 'file_name',\n",
       " 'files_changed',\n",
       " 'func_after',\n",
       " 'func_before',\n",
       " 'lang',\n",
       " 'lines_after',\n",
       " 'lines_before',\n",
       " 'parentID',\n",
       " 'patch',\n",
       " 'project',\n",
       " 'project_after',\n",
       " 'project_before',\n",
       " 'target',\n",
       " 'vul_func_with_fix',\n",
       " 'processed_func',\n",
       " 'flaw_line',\n",
       " 'flaw_line_index']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_row = test_data.sample(n=1)\n",
    "text = random_row['func_before'].values[0]\n",
    "random_row['Score'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'void CL_Init( void ) {\\n\\tCom_Printf( \"----- Client Initialization -----\\\\n\" );\\n\\n\\tCon_Init ();\\n\\n\\tif(!com_fullyInitialized)\\n\\t{\\n\\t\\tCL_ClearState();\\n\\t\\tclc.state = CA_DISCONNECTED;\\t// no longer CA_UNINITIALIZED\\n\\t\\tcl_oldGameSet = qfalse;\\n\\t}\\n\\n\\tcls.realtime = 0;\\n\\n\\tCL_InitInput ();\\n\\n\\tcl_noprint = Cvar_Get( \"cl_noprint\", \"0\", 0 );\\n#ifdef UPDATE_SERVER_NAME\\n\\tcl_motd = Cvar_Get (\"cl_motd\", \"1\", 0);\\n#endif\\n\\n\\tcl_timeout = Cvar_Get (\"cl_timeout\", \"200\", 0);\\n\\n\\tcl_timeNudge = Cvar_Get (\"cl_timeNudge\", \"0\", CVAR_TEMP );\\n\\tcl_shownet = Cvar_Get (\"cl_shownet\", \"0\", CVAR_TEMP );\\n\\tcl_showSend = Cvar_Get (\"cl_showSend\", \"0\", CVAR_TEMP );\\n\\tcl_showTimeDelta = Cvar_Get (\"cl_showTimeDelta\", \"0\", CVAR_TEMP );\\n\\tcl_freezeDemo = Cvar_Get (\"cl_freezeDemo\", \"0\", CVAR_TEMP );\\n\\trcon_client_password = Cvar_Get (\"rconPassword\", \"\", CVAR_TEMP );\\n\\tcl_activeAction = Cvar_Get( \"activeAction\", \"\", CVAR_TEMP );\\n\\n\\tcl_timedemo = Cvar_Get (\"timedemo\", \"0\", 0);\\n\\tcl_timedemoLog = Cvar_Get (\"cl_timedemoLog\", \"\", CVAR_ARCHIVE);\\n\\tcl_autoRecordDemo = Cvar_Get (\"cl_autoRecordDemo\", \"0\", CVAR_ARCHIVE);\\n\\tcl_aviFrameRate = Cvar_Get (\"cl_aviFrameRate\", \"25\", CVAR_ARCHIVE);\\n\\tcl_aviMotionJpeg = Cvar_Get (\"cl_aviMotionJpeg\", \"1\", CVAR_ARCHIVE);\\n\\tcl_forceavidemo = Cvar_Get (\"cl_forceavidemo\", \"0\", 0);\\n\\n\\trconAddress = Cvar_Get (\"rconAddress\", \"\", 0);\\n\\n\\tcl_yawspeed = Cvar_Get (\"cl_yawspeed\", \"140\", CVAR_ARCHIVE);\\n\\tcl_pitchspeed = Cvar_Get (\"cl_pitchspeed\", \"140\", CVAR_ARCHIVE);\\n\\tcl_anglespeedkey = Cvar_Get (\"cl_anglespeedkey\", \"1.5\", 0);\\n\\n\\tcl_maxpackets = Cvar_Get (\"cl_maxpackets\", \"30\", CVAR_ARCHIVE );\\n\\tcl_packetdup = Cvar_Get (\"cl_packetdup\", \"1\", CVAR_ARCHIVE );\\n\\n\\tcl_run = Cvar_Get (\"cl_run\", \"1\", CVAR_ARCHIVE);\\n\\tcl_sensitivity = Cvar_Get (\"sensitivity\", \"5\", CVAR_ARCHIVE);\\n\\tcl_mouseAccel = Cvar_Get (\"cl_mouseAccel\", \"0\", CVAR_ARCHIVE);\\n\\tcl_freelook = Cvar_Get( \"cl_freelook\", \"1\", CVAR_ARCHIVE );\\n\\n\\tcl_mouseAccelStyle = Cvar_Get( \"cl_mouseAccelStyle\", \"0\", CVAR_ARCHIVE );\\n\\tcl_mouseAccelOffset = Cvar_Get( \"cl_mouseAccelOffset\", \"5\", CVAR_ARCHIVE );\\n\\tCvar_CheckRange(cl_mouseAccelOffset, 0.001f, 50000.0f, qfalse);\\n\\n\\tcl_showMouseRate = Cvar_Get (\"cl_showmouserate\", \"0\", 0);\\n \\n \\tcl_allowDownload = Cvar_Get (\"cl_allowDownload\", \"0\", CVAR_ARCHIVE);\\n #ifdef USE_CURL_DLOPEN\\n\\tcl_cURLLib = Cvar_Get(\"cl_cURLLib\", DEFAULT_CURL_LIB, CVAR_ARCHIVE);\\n #endif\\n \\n \\tcl_conXOffset = Cvar_Get (\"cl_conXOffset\", \"0\", 0);\\n#ifdef __APPLE__\\n\\tcl_inGameVideo = Cvar_Get (\"r_inGameVideo\", \"0\", CVAR_ARCHIVE);\\n#else\\n\\tcl_inGameVideo = Cvar_Get (\"r_inGameVideo\", \"1\", CVAR_ARCHIVE);\\n#endif\\n\\n\\tcl_serverStatusResendTime = Cvar_Get (\"cl_serverStatusResendTime\", \"750\", 0);\\n\\n\\tCvar_Get (\"cg_autoswitch\", \"1\", CVAR_ARCHIVE);\\n\\n\\tm_pitch = Cvar_Get (\"m_pitch\", \"0.022\", CVAR_ARCHIVE);\\n\\tm_yaw = Cvar_Get (\"m_yaw\", \"0.022\", CVAR_ARCHIVE);\\n\\tm_forward = Cvar_Get (\"m_forward\", \"0.25\", CVAR_ARCHIVE);\\n\\tm_side = Cvar_Get (\"m_side\", \"0.25\", CVAR_ARCHIVE);\\n#ifdef __APPLE__\\n\\tm_filter = Cvar_Get (\"m_filter\", \"1\", CVAR_ARCHIVE);\\n#else\\n\\tm_filter = Cvar_Get (\"m_filter\", \"0\", CVAR_ARCHIVE);\\n#endif\\n\\n\\tj_pitch =        Cvar_Get (\"j_pitch\",        \"0.022\", CVAR_ARCHIVE);\\n\\tj_yaw =          Cvar_Get (\"j_yaw\",          \"-0.022\", CVAR_ARCHIVE);\\n\\tj_forward =      Cvar_Get (\"j_forward\",      \"-0.25\", CVAR_ARCHIVE);\\n\\tj_side =         Cvar_Get (\"j_side\",         \"0.25\", CVAR_ARCHIVE);\\n\\tj_up =           Cvar_Get (\"j_up\",           \"0\", CVAR_ARCHIVE);\\n\\n\\tj_pitch_axis =   Cvar_Get (\"j_pitch_axis\",   \"3\", CVAR_ARCHIVE);\\n\\tj_yaw_axis =     Cvar_Get (\"j_yaw_axis\",     \"2\", CVAR_ARCHIVE);\\n\\tj_forward_axis = Cvar_Get (\"j_forward_axis\", \"1\", CVAR_ARCHIVE);\\n\\tj_side_axis =    Cvar_Get (\"j_side_axis\",    \"0\", CVAR_ARCHIVE);\\n\\tj_up_axis =      Cvar_Get (\"j_up_axis\",      \"4\", CVAR_ARCHIVE);\\n\\n\\tCvar_CheckRange(j_pitch_axis, 0, MAX_JOYSTICK_AXIS-1, qtrue);\\n\\tCvar_CheckRange(j_yaw_axis, 0, MAX_JOYSTICK_AXIS-1, qtrue);\\n\\tCvar_CheckRange(j_forward_axis, 0, MAX_JOYSTICK_AXIS-1, qtrue);\\n\\tCvar_CheckRange(j_side_axis, 0, MAX_JOYSTICK_AXIS-1, qtrue);\\n\\tCvar_CheckRange(j_up_axis, 0, MAX_JOYSTICK_AXIS-1, qtrue);\\n\\n\\tcl_motdString = Cvar_Get( \"cl_motdString\", \"\", CVAR_ROM );\\n\\n\\tCvar_Get( \"cl_maxPing\", \"800\", CVAR_ARCHIVE );\\n\\n\\tcl_lanForcePackets = Cvar_Get (\"cl_lanForcePackets\", \"1\", CVAR_ARCHIVE);\\n\\n\\tcl_guidServerUniq = Cvar_Get (\"cl_guidServerUniq\", \"1\", CVAR_ARCHIVE);\\n\\n\\tcl_consoleKeys = Cvar_Get( \"cl_consoleKeys\", \"~ ` 0x7e 0x60\", CVAR_ARCHIVE);\\n\\n\\tCvar_Get (\"name\", \"UnnamedPlayer\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tcl_rate = Cvar_Get (\"rate\", \"25000\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"snaps\", \"20\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"model\", \"sarge\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"headmodel\", \"sarge\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"team_model\", \"james\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"team_headmodel\", \"*james\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"g_redTeam\", \"Stroggs\", CVAR_SERVERINFO | CVAR_ARCHIVE);\\n\\tCvar_Get (\"g_blueTeam\", \"Pagans\", CVAR_SERVERINFO | CVAR_ARCHIVE);\\n\\tCvar_Get (\"color1\",  \"4\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"color2\", \"5\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"handicap\", \"100\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"teamtask\", \"0\", CVAR_USERINFO );\\n\\tCvar_Get (\"sex\", \"male\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\tCvar_Get (\"cl_anonymous\", \"0\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\n\\tCvar_Get (\"password\", \"\", CVAR_USERINFO);\\n\\tCvar_Get (\"cg_predictItems\", \"1\", CVAR_USERINFO | CVAR_ARCHIVE );\\n\\n#ifdef USE_MUMBLE\\n\\tcl_useMumble = Cvar_Get (\"cl_useMumble\", \"0\", CVAR_ARCHIVE | CVAR_LATCH);\\n\\tcl_mumbleScale = Cvar_Get (\"cl_mumbleScale\", \"0.0254\", CVAR_ARCHIVE);\\n#endif\\n\\n#ifdef USE_VOIP\\n\\tcl_voipSend = Cvar_Get (\"cl_voipSend\", \"0\", 0);\\n\\tcl_voipSendTarget = Cvar_Get (\"cl_voipSendTarget\", \"spatial\", 0);\\n\\tcl_voipGainDuringCapture = Cvar_Get (\"cl_voipGainDuringCapture\", \"0.2\", CVAR_ARCHIVE);\\n\\tcl_voipCaptureMult = Cvar_Get (\"cl_voipCaptureMult\", \"2.0\", CVAR_ARCHIVE);\\n\\tcl_voipUseVAD = Cvar_Get (\"cl_voipUseVAD\", \"0\", CVAR_ARCHIVE);\\n\\tcl_voipVADThreshold = Cvar_Get (\"cl_voipVADThreshold\", \"0.25\", CVAR_ARCHIVE);\\n\\tcl_voipShowMeter = Cvar_Get (\"cl_voipShowMeter\", \"1\", CVAR_ARCHIVE);\\n\\n\\tcl_voip = Cvar_Get (\"cl_voip\", \"1\", CVAR_ARCHIVE);\\n\\tCvar_CheckRange( cl_voip, 0, 1, qtrue );\\n\\tcl_voipProtocol = Cvar_Get (\"cl_voipProtocol\", cl_voip->integer ? \"opus\" : \"\", CVAR_USERINFO | CVAR_ROM);\\n#endif\\n\\n\\n\\tCvar_Get (\"cg_viewsize\", \"100\", CVAR_ARCHIVE );\\n\\tCvar_Get (\"cg_stereoSeparation\", \"0\", CVAR_ROM);\\n\\n\\tCmd_AddCommand (\"cmd\", CL_ForwardToServer_f);\\n\\tCmd_AddCommand (\"configstrings\", CL_Configstrings_f);\\n\\tCmd_AddCommand (\"clientinfo\", CL_Clientinfo_f);\\n\\tCmd_AddCommand (\"snd_restart\", CL_Snd_Restart_f);\\n\\tCmd_AddCommand (\"vid_restart\", CL_Vid_Restart_f);\\n\\tCmd_AddCommand (\"disconnect\", CL_Disconnect_f);\\n\\tCmd_AddCommand (\"record\", CL_Record_f);\\n\\tCmd_AddCommand (\"demo\", CL_PlayDemo_f);\\n\\tCmd_SetCommandCompletionFunc( \"demo\", CL_CompleteDemoName );\\n\\tCmd_AddCommand (\"cinematic\", CL_PlayCinematic_f);\\n\\tCmd_AddCommand (\"stoprecord\", CL_StopRecord_f);\\n\\tCmd_AddCommand (\"connect\", CL_Connect_f);\\n\\tCmd_AddCommand (\"reconnect\", CL_Reconnect_f);\\n\\tCmd_AddCommand (\"localservers\", CL_LocalServers_f);\\n\\tCmd_AddCommand (\"globalservers\", CL_GlobalServers_f);\\n\\tCmd_AddCommand (\"rcon\", CL_Rcon_f);\\n\\tCmd_SetCommandCompletionFunc( \"rcon\", CL_CompleteRcon );\\n\\tCmd_AddCommand (\"ping\", CL_Ping_f );\\n\\tCmd_AddCommand (\"serverstatus\", CL_ServerStatus_f );\\n\\tCmd_AddCommand (\"showip\", CL_ShowIP_f );\\n\\tCmd_AddCommand (\"fs_openedList\", CL_OpenedPK3List_f );\\n\\tCmd_AddCommand (\"fs_referencedList\", CL_ReferencedPK3List_f );\\n\\tCmd_AddCommand (\"model\", CL_SetModel_f );\\n\\tCmd_AddCommand (\"video\", CL_Video_f );\\n\\tCmd_AddCommand (\"stopvideo\", CL_StopVideo_f );\\n\\tif( !com_dedicated->integer ) {\\n\\t\\tCmd_AddCommand (\"sayto\", CL_Sayto_f );\\n\\t\\tCmd_SetCommandCompletionFunc( \"sayto\", CL_CompletePlayerName );\\n\\t}\\n\\tCL_InitRef();\\n\\n\\tSCR_Init ();\\n\\n\\n\\tCvar_Set( \"cl_running\", \"1\" );\\n\\n\\tCL_GenerateQKey();\\n\\tCvar_Get( \"cl_guid\", \"\", CVAR_USERINFO | CVAR_ROM );\\n\\tCL_UpdateGUID( NULL, 0 );\\n\\n\\tCom_Printf( \"----- Client Initialization Complete -----\\\\n\" );\\n}\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages():\n",
    "    messages = []\n",
    "    indexes = []\n",
    "    gt = None\n",
    "    for i in range(MESSAGES_LENGTH):\n",
    "        random_row = filtered_val.sample(n=1)\n",
    "        text = random_row['func_before'].values[0]\n",
    "        score = random_row['Score'].values[0]\n",
    "        message = p1_template.format(score, text)\n",
    "        indexes.append(random_row['index'].values[0])\n",
    "        messages.append(message)\n",
    "    random_row = filtered_test.sample(n=1)\n",
    "    text = random_row['func_before'].values[0]\n",
    "    gt = random_row['Score'].values[0]\n",
    "    message = p2_template.format(text)\n",
    "    indexes.append(random_row['index'].values[0])\n",
    "    messages.append(message)\n",
    "    return messages, indexes, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages, indexes, gt_score = build_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120304, 1267]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit maximum iterations for conversation\n",
    "def generate_prediction(messages):\n",
    "    conversation_history = list()\n",
    "\n",
    "    for message in messages:\n",
    "\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "        conversation_history.append({\"role\": \"system\", \"content\": \"\"})\n",
    "        # Convert conversational history into chat template and tokenize\n",
    "        inputs = tokenizer.apply_chat_template(conversation_history, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = model.generate(inputs,\n",
    "            #streamer=streamer,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.92,\n",
    "            temperature= 0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Get complete output from model including input prompt\n",
    "        output = tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "        # Filter only new output information using '</s>' delimiter, then strip starting and trailing whitespace\n",
    "        output_filtered = output.split('[/INST]')[-1].strip()\n",
    "\n",
    "        # Update conversation history with the latest output\n",
    "        conversation_history[-1][\"content\"] = output_filtered\n",
    "\n",
    "    return conversation_history\n",
    "\n",
    "        # Capture input before start of next iteration\n",
    "        #capture_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trials():\n",
    "    conversations = list()\n",
    "    messages, indexes, gt_score = build_messages()\n",
    "    for i in range(max_iterations):\n",
    "        conversation = generate_prediction(messages)\n",
    "        conversations.append(conversation)\n",
    "    return conversations, indexes, gt_score\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations, indexes, gt_score = generate_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversations(conversation_history):\n",
    "    # Save entire conversation history to text file for debugging or use for loading conversational context\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(conversation_history, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(SAMPLES):\n",
    "    conversations, indexes, gt_score = generate_trials()\n",
    "    conversations.append({\"indexes\":indexes, \"gt_score\":gt_score})\n",
    "    result.append(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}_100_10_codeLlama7b.txt\"\n",
    "save_conversations(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load conversational history from a previous context file\n",
    "context_filename = \"./*.txt\"\n",
    "with open(context_filename, 'r') as f:\n",
    "     data = json.load(f)\n",
    "     conversation_history = data\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
